{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSE 2022: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T16:48:20.566549Z",
     "start_time": "2020-09-26T16:48:19.893995Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLSResults\n",
    "from math import sqrt\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "For this homework we use Dataset from seaborn on diamonds prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53940 entries, 0 to 53939\n",
      "Data columns (total 10 columns):\n",
      " #   Column   Non-Null Count  Dtype   \n",
      "---  ------   --------------  -----   \n",
      " 0   carat    53940 non-null  float64 \n",
      " 1   cut      53940 non-null  category\n",
      " 2   color    53940 non-null  category\n",
      " 3   clarity  53940 non-null  category\n",
      " 4   depth    53940 non-null  float64 \n",
      " 5   table    53940 non-null  float64 \n",
      " 6   price    53940 non-null  int64   \n",
      " 7   x        53940 non-null  float64 \n",
      " 8   y        53940 non-null  float64 \n",
      " 9   z        53940 non-null  float64 \n",
      "dtypes: category(3), float64(6), int64(1)\n",
      "memory usage: 3.0 MB\n"
     ]
    }
   ],
   "source": [
    "data = sns.load_dataset('diamonds')\n",
    "\n",
    "y = data.price\n",
    "X = data.drop(['price'], axis=1)\n",
    "columns = data.drop(['price'], axis=1).columns\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. [0.25 points] Encode categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "   carat  cut  color  clarity  depth  table     x     y     z\n0   0.23    2      1        3   61.5   55.0  3.95  3.98  2.43\n1   0.21    3      1        2   59.8   61.0  3.89  3.84  2.31\n2   0.23    1      1        4   56.9   65.0  4.05  4.07  2.31\n3   0.29    3      5        5   62.4   58.0  4.20  4.23  2.63\n4   0.31    1      6        3   63.3   58.0  4.34  4.35  2.75",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>carat</th>\n      <th>cut</th>\n      <th>color</th>\n      <th>clarity</th>\n      <th>depth</th>\n      <th>table</th>\n      <th>x</th>\n      <th>y</th>\n      <th>z</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.23</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>61.5</td>\n      <td>55.0</td>\n      <td>3.95</td>\n      <td>3.98</td>\n      <td>2.43</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.21</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>59.8</td>\n      <td>61.0</td>\n      <td>3.89</td>\n      <td>3.84</td>\n      <td>2.31</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.23</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>56.9</td>\n      <td>65.0</td>\n      <td>4.05</td>\n      <td>4.07</td>\n      <td>2.31</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.29</td>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>62.4</td>\n      <td>58.0</td>\n      <td>4.20</td>\n      <td>4.23</td>\n      <td>2.63</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.31</td>\n      <td>1</td>\n      <td>6</td>\n      <td>3</td>\n      <td>63.3</td>\n      <td>58.0</td>\n      <td>4.34</td>\n      <td>4.35</td>\n      <td>2.75</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "categorical = list(X.dtypes[X.dtypes == \"category\"].index)\n",
    "X[categorical] = X[categorical].fillna(\"NotGiven\")\n",
    "X[categorical] = X[categorical].fillna(\"NotGiven\")\n",
    "X['cut'] = encoder.fit_transform(X.cut)\n",
    "X['color'] = encoder.fit_transform(X.color)\n",
    "X['clarity'] = encoder.fit_transform(X.clarity)\n",
    "X.head()\n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. [0.25 points] Split the data into train and test sets with ratio 80:20 with random_state=17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17)\n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. [1 point] Train models on train data using StatsModels library and apply it to the test set; use $RMSE$ and $R^2$ as the quality measure.\n",
    "\n",
    "* [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html);\n",
    "* [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with $\\alpha = 0.01$;\n",
    "* [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) with $\\alpha = 0.01$\n",
    "* [`ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) with $\\alpha = 0.01$, $l_{1}$_$ratio = 0.6$\n",
    "\n",
    "Don't forget to scale the data before training the models with StandardScaler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 1347.9933\n",
      "Test RMSE = 1370.5909\n",
      "Train R2 = 0.8853\n",
      "Test R2 = 0.8840\n"
     ]
    },
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.885      \nDependent Variable: price            AIC:                744418.8263\nDate:               2022-10-16 20:26 BIC:                744505.5512\nNo. Observations:   43152            Log-Likelihood:     -3.7220e+05\nDf Model:           9                F-statistic:        3.701e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.885            Scale:              1.8175e+06 \n---------------------------------------------------------------------\n         Coef.     Std.Err.     t      P>|t|     [0.025      0.975]  \n---------------------------------------------------------------------\nconst   3928.6813    6.4899  605.3537  0.0000   3915.9610   3941.4016\nx1      5257.1453   31.0710  169.1979  0.0000   5196.2456   5318.0450\nx2        76.4610    6.6590   11.4824  0.0000     63.4093     89.5128\nx3      -455.4350    6.8100  -66.8778  0.0000   -468.7826   -442.0874\nx4       491.4240    6.7022   73.3231  0.0000    478.2876    504.5604\nx5      -226.2704    7.9533  -28.4498  0.0000   -241.8590   -210.6818\nx6      -213.2612    6.9923  -30.4996  0.0000   -226.9662   -199.5562\nx7     -1383.2878   48.3537  -28.6077  0.0000  -1478.0619  -1288.5137\nx8        42.1665   29.5137    1.4287  0.1531    -15.6809    100.0138\nx9         3.3540   29.6459    0.1131  0.9099    -54.7524     61.4604\n--------------------------------------------------------------------\nOmnibus:            11265.146      Durbin-Watson:         1.999     \nProb(Omnibus):      0.000          Jarque-Bera (JB):      360275.892\nSkew:               0.611          Prob(JB):              0.000     \nKurtosis:           17.103         Condition No.:         18        \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.885</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>744418.8263</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-10-16 20:26</td>        <td>BIC:</td>         <td>744505.5512</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.7220e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>3.701e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.885</td>            <td>Scale:</td>        <td>1.8175e+06</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>   <th>[0.025</th>     <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>  <td>3928.6813</td>  <td>6.4899</td>  <td>605.3537</td> <td>0.0000</td>  <td>3915.9610</td>  <td>3941.4016</td>\n</tr>\n<tr>\n  <th>x1</th>     <td>5257.1453</td>  <td>31.0710</td> <td>169.1979</td> <td>0.0000</td>  <td>5196.2456</td>  <td>5318.0450</td>\n</tr>\n<tr>\n  <th>x2</th>      <td>76.4610</td>   <td>6.6590</td>   <td>11.4824</td> <td>0.0000</td>   <td>63.4093</td>    <td>89.5128</td> \n</tr>\n<tr>\n  <th>x3</th>     <td>-455.4350</td>  <td>6.8100</td>  <td>-66.8778</td> <td>0.0000</td>  <td>-468.7826</td>  <td>-442.0874</td>\n</tr>\n<tr>\n  <th>x4</th>     <td>491.4240</td>   <td>6.7022</td>   <td>73.3231</td> <td>0.0000</td>  <td>478.2876</td>   <td>504.5604</td> \n</tr>\n<tr>\n  <th>x5</th>     <td>-226.2704</td>  <td>7.9533</td>  <td>-28.4498</td> <td>0.0000</td>  <td>-241.8590</td>  <td>-210.6818</td>\n</tr>\n<tr>\n  <th>x6</th>     <td>-213.2612</td>  <td>6.9923</td>  <td>-30.4996</td> <td>0.0000</td>  <td>-226.9662</td>  <td>-199.5562</td>\n</tr>\n<tr>\n  <th>x7</th>    <td>-1383.2878</td>  <td>48.3537</td> <td>-28.6077</td> <td>0.0000</td> <td>-1478.0619</td> <td>-1288.5137</td>\n</tr>\n<tr>\n  <th>x8</th>      <td>42.1665</td>   <td>29.5137</td>  <td>1.4287</td>  <td>0.1531</td>  <td>-15.6809</td>   <td>100.0138</td> \n</tr>\n<tr>\n  <th>x9</th>      <td>3.3540</td>    <td>29.6459</td>  <td>0.1131</td>  <td>0.9099</td>  <td>-54.7524</td>    <td>61.4604</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>11265.146</td>  <td>Durbin-Watson:</td>      <td>1.999</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>360275.892</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>0.611</td>       <td>Prob(JB):</td>        <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>    <td>17.103</td>    <td>Condition No.:</td>       <td>18</td>    \n</tr>\n</table>"
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled = sm.add_constant(X_train_scaled)\n",
    "X_test_scaled = sm.add_constant(X_test_scaled)\n",
    "model = sm.OLS(y_train, X_train_scaled)\n",
    "results = model.fit()\n",
    "y_pred_linear = results.predict(X_test_scaled)\n",
    "y_train_pred_linear = results.predict(X_train_scaled)\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_pred_linear, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_pred_linear, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_pred_linear))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_pred_linear))\n",
    "OLSResults(model, results.params, model.normalized_cov_params).summary2()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 1365.9920\n",
      "Test RMSE = 1380.9027\n",
      "Train R2 = 0.8822\n",
      "Test R2 = 0.8822\n"
     ]
    },
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.882\nModel:                            OLS   Adj. R-squared:                  0.882\nMethod:                 Least Squares   F-statistic:                 3.591e+04\nDate:                Sun, 16 Oct 2022   Prob (F-statistic):               0.00\nTime:                        20:26:14   Log-Likelihood:            -3.7277e+05\nNo. Observations:               43152   AIC:                         7.456e+05\nDf Residuals:                   43142   BIC:                         7.457e+05\nDf Model:                           9                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       3889.7835      6.577    591.463      0.000    3876.893    3902.674\nx1          4219.7372     31.486    134.020      0.000    4158.024    4281.450\nx2            81.8556      6.748     12.131      0.000      68.630      95.082\nx3          -424.7483      6.901    -61.550      0.000    -438.274    -411.222\nx4           496.6610      6.792     73.128      0.000     483.349     509.973\nx5          -162.6547      8.060    -20.182      0.000    -178.451    -146.858\nx6          -203.6562      7.086    -28.742      0.000    -217.544    -189.768\nx7          -312.9137     48.999     -6.386      0.000    -408.953    -216.874\nx8            21.4162     29.908      0.716      0.474     -37.204      80.036\nx9           -39.9802     30.042     -1.331      0.183     -98.862      18.902\n==============================================================================\nOmnibus:                    13066.102   Durbin-Watson:                   1.998\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           183804.270\nSkew:                           1.067   Prob(JB):                         0.00\nKurtosis:                      12.883   Cond. No.                         17.5\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th>  <td>   0.882</td>  \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.882</td>  \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>3.591e+04</td> \n</tr>\n<tr>\n  <th>Date:</th>             <td>Sun, 16 Oct 2022</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n</tr>\n<tr>\n  <th>Time:</th>                 <td>20:26:14</td>     <th>  Log-Likelihood:    </th> <td>-3.7277e+05</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td> 43152</td>      <th>  AIC:               </th>  <td>7.456e+05</td> \n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td> 43142</td>      <th>  BIC:               </th>  <td>7.457e+05</td> \n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     9</td>      <th>                     </th>      <td> </td>     \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td> 3889.7835</td> <td>    6.577</td> <td>  591.463</td> <td> 0.000</td> <td> 3876.893</td> <td> 3902.674</td>\n</tr>\n<tr>\n  <th>x1</th>    <td> 4219.7372</td> <td>   31.486</td> <td>  134.020</td> <td> 0.000</td> <td> 4158.024</td> <td> 4281.450</td>\n</tr>\n<tr>\n  <th>x2</th>    <td>   81.8556</td> <td>    6.748</td> <td>   12.131</td> <td> 0.000</td> <td>   68.630</td> <td>   95.082</td>\n</tr>\n<tr>\n  <th>x3</th>    <td> -424.7483</td> <td>    6.901</td> <td>  -61.550</td> <td> 0.000</td> <td> -438.274</td> <td> -411.222</td>\n</tr>\n<tr>\n  <th>x4</th>    <td>  496.6610</td> <td>    6.792</td> <td>   73.128</td> <td> 0.000</td> <td>  483.349</td> <td>  509.973</td>\n</tr>\n<tr>\n  <th>x5</th>    <td> -162.6547</td> <td>    8.060</td> <td>  -20.182</td> <td> 0.000</td> <td> -178.451</td> <td> -146.858</td>\n</tr>\n<tr>\n  <th>x6</th>    <td> -203.6562</td> <td>    7.086</td> <td>  -28.742</td> <td> 0.000</td> <td> -217.544</td> <td> -189.768</td>\n</tr>\n<tr>\n  <th>x7</th>    <td> -312.9137</td> <td>   48.999</td> <td>   -6.386</td> <td> 0.000</td> <td> -408.953</td> <td> -216.874</td>\n</tr>\n<tr>\n  <th>x8</th>    <td>   21.4162</td> <td>   29.908</td> <td>    0.716</td> <td> 0.474</td> <td>  -37.204</td> <td>   80.036</td>\n</tr>\n<tr>\n  <th>x9</th>    <td>  -39.9802</td> <td>   30.042</td> <td>   -1.331</td> <td> 0.183</td> <td>  -98.862</td> <td>   18.902</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>13066.102</td> <th>  Durbin-Watson:     </th>  <td>   1.998</td> \n</tr>\n<tr>\n  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>183804.270</td>\n</tr>\n<tr>\n  <th>Skew:</th>           <td> 1.067</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> \n</tr>\n<tr>\n  <th>Kurtosis:</th>       <td>12.883</td>   <th>  Cond. No.          </th>  <td>    17.5</td> \n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.tools.tools import pinv_extended\n",
    "\n",
    "X_train_scaled = sm.add_constant(X_train_scaled)\n",
    "X_test_scaled = sm.add_constant(X_test_scaled)\n",
    "model = sm.OLS(y_train, X_train_scaled)\n",
    "results = model.fit_regularized(L1_wt=0, alpha=0.01)\n",
    "y_pred_ridge = results.predict(X_test_scaled)\n",
    "y_train_pred_ridge = results.predict(X_train_scaled)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_pred_ridge, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_pred_ridge, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_pred_ridge))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_pred_ridge))\n",
    "pinv_wexog, _ = pinv_extended(model.wexog)\n",
    "normalized_cov_params = np.dot(pinv_wexog, np.transpose(pinv_wexog))\n",
    "OLSResults(model, results.params, normalized_cov_params).summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 1348.6037\n",
      "Test RMSE = 1369.1877\n",
      "Train R2 = 0.8852\n",
      "Test R2 = 0.8842\n"
     ]
    },
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.885      \nDependent Variable: price            AIC:                744457.9003\nDate:               2022-10-16 20:26 BIC:                744544.6252\nNo. Observations:   43152            Log-Likelihood:     -3.7222e+05\nDf Model:           9                F-statistic:        3.697e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.885            Scale:              1.8192e+06 \n---------------------------------------------------------------------\n         Coef.     Std.Err.     t      P>|t|     [0.025      0.975]  \n---------------------------------------------------------------------\nconst   3928.6713    6.4928  605.0782  0.0000   3915.9452   3941.3974\nx1      5063.8698   31.0850  162.9037  0.0000   5002.9425   5124.7970\nx2        77.5642    6.6620   11.6428  0.0000     64.5066     90.6218\nx3      -453.1266    6.8130  -66.5087  0.0000   -466.4802   -439.7729\nx4       495.2762    6.7052   73.8644  0.0000    482.1339    508.4186\nx5      -214.9442    7.9569  -27.0135  0.0000   -230.5399   -199.3485\nx6      -213.4343    6.9954  -30.5105  0.0000   -227.1455   -199.7231\nx7     -1201.4586   48.3756  -24.8361  0.0000  -1296.2757  -1106.6416\nx8        54.7441   29.5270    1.8540  0.0637     -3.1294    112.6176\nx9        -1.4625   29.6593   -0.0493  0.9607    -59.5953     56.6702\n--------------------------------------------------------------------\nOmnibus:            11656.263      Durbin-Watson:         1.999     \nProb(Omnibus):      0.000          Jarque-Bera (JB):      315053.588\nSkew:               0.713          Prob(JB):              0.000     \nKurtosis:           16.160         Condition No.:         18        \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.885</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>744457.9003</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-10-16 20:26</td>        <td>BIC:</td>         <td>744544.6252</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.7222e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>3.697e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.885</td>            <td>Scale:</td>        <td>1.8192e+06</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>   <th>[0.025</th>     <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>  <td>3928.6713</td>  <td>6.4928</td>  <td>605.0782</td> <td>0.0000</td>  <td>3915.9452</td>  <td>3941.3974</td>\n</tr>\n<tr>\n  <th>x1</th>     <td>5063.8698</td>  <td>31.0850</td> <td>162.9037</td> <td>0.0000</td>  <td>5002.9425</td>  <td>5124.7970</td>\n</tr>\n<tr>\n  <th>x2</th>      <td>77.5642</td>   <td>6.6620</td>   <td>11.6428</td> <td>0.0000</td>   <td>64.5066</td>    <td>90.6218</td> \n</tr>\n<tr>\n  <th>x3</th>     <td>-453.1266</td>  <td>6.8130</td>  <td>-66.5087</td> <td>0.0000</td>  <td>-466.4802</td>  <td>-439.7729</td>\n</tr>\n<tr>\n  <th>x4</th>     <td>495.2762</td>   <td>6.7052</td>   <td>73.8644</td> <td>0.0000</td>  <td>482.1339</td>   <td>508.4186</td> \n</tr>\n<tr>\n  <th>x5</th>     <td>-214.9442</td>  <td>7.9569</td>  <td>-27.0135</td> <td>0.0000</td>  <td>-230.5399</td>  <td>-199.3485</td>\n</tr>\n<tr>\n  <th>x6</th>     <td>-213.4343</td>  <td>6.9954</td>  <td>-30.5105</td> <td>0.0000</td>  <td>-227.1455</td>  <td>-199.7231</td>\n</tr>\n<tr>\n  <th>x7</th>    <td>-1201.4586</td>  <td>48.3756</td> <td>-24.8361</td> <td>0.0000</td> <td>-1296.2757</td> <td>-1106.6416</td>\n</tr>\n<tr>\n  <th>x8</th>      <td>54.7441</td>   <td>29.5270</td>  <td>1.8540</td>  <td>0.0637</td>   <td>-3.1294</td>   <td>112.6176</td> \n</tr>\n<tr>\n  <th>x9</th>      <td>-1.4625</td>   <td>29.6593</td>  <td>-0.0493</td> <td>0.9607</td>  <td>-59.5953</td>    <td>56.6702</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>11656.263</td>  <td>Durbin-Watson:</td>      <td>1.999</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>315053.588</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>0.713</td>       <td>Prob(JB):</td>        <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>    <td>16.160</td>    <td>Condition No.:</td>       <td>18</td>    \n</tr>\n</table>"
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled = sm.add_constant(X_train_scaled)\n",
    "X_test_scaled = sm.add_constant(X_test_scaled)\n",
    "model = sm.OLS(y_train, X_train_scaled)\n",
    "results = model.fit_regularized(L1_wt=1, alpha=0.01)\n",
    "\n",
    "y_pred_lasso = results.predict(X_test_scaled)\n",
    "y_train_pred_lasso = results.predict(X_train_scaled)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_pred_lasso, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_pred_lasso, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_pred_lasso))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_pred_lasso))\n",
    "pinv_wexog, _ = pinv_extended(model.wexog)\n",
    "normalized_cov_params = np.dot(pinv_wexog, np.transpose(pinv_wexog))\n",
    "OLSResults(model, results.params, normalized_cov_params).summary2()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 1353.6044\n",
      "Test RMSE = 1370.9586\n",
      "Train R2 = 0.8844\n",
      "Test R2 = 0.8839\n"
     ]
    },
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.884      \nDependent Variable: price            AIC:                744777.3304\nDate:               2022-10-16 20:26 BIC:                744864.0552\nNo. Observations:   43152            Log-Likelihood:     -3.7238e+05\nDf Model:           9                F-statistic:        3.666e+04  \nDf Residuals:       43142            Prob (F-statistic): 0.00       \nR-squared:          0.884            Scale:              1.8327e+06 \n---------------------------------------------------------------------\n            Coef.    Std.Err.     t      P>|t|     [0.025     0.975] \n---------------------------------------------------------------------\nconst     3913.0232    6.5169  600.4416  0.0000  3900.2499  3925.7965\nx1        4675.8946   31.2003  149.8669  0.0000  4614.7414  4737.0478\nx2          79.7319    6.6867   11.9240  0.0000    66.6258    92.8379\nx3        -440.5288    6.8383  -64.4208  0.0000  -453.9320  -427.1256\nx4         496.4432    6.7301   73.7649  0.0000   483.2521   509.6342\nx5        -190.1759    7.9864  -23.8124  0.0000  -205.8294  -174.5223\nx6        -209.3948    7.0214  -29.8225  0.0000  -223.1568  -195.6328\nx7        -766.0692   48.5550  -15.7774  0.0000  -861.2378  -670.9005\nx8          20.1115   29.6365    0.6786  0.4974   -37.9767    78.1996\nx9         -26.8075   29.7693   -0.9005  0.3679   -85.1558    31.5408\n--------------------------------------------------------------------\nOmnibus:            12406.161      Durbin-Watson:         1.999     \nProb(Omnibus):      0.000          Jarque-Bera (JB):      244618.648\nSkew:               0.894          Prob(JB):              0.000     \nKurtosis:           14.526         Condition No.:         18        \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.884</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>744777.3304</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-10-16 20:26</td>        <td>BIC:</td>         <td>744864.0552</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.7238e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>9</td>           <td>F-statistic:</td>      <td>3.666e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43142</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.884</td>            <td>Scale:</td>        <td>1.8327e+06</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>      <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td>3913.0232</td>  <td>6.5169</td>  <td>600.4416</td> <td>0.0000</td> <td>3900.2499</td> <td>3925.7965</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>4675.8946</td>  <td>31.2003</td> <td>149.8669</td> <td>0.0000</td> <td>4614.7414</td> <td>4737.0478</td>\n</tr>\n<tr>\n  <th>x2</th>     <td>79.7319</td>   <td>6.6867</td>   <td>11.9240</td> <td>0.0000</td>  <td>66.6258</td>   <td>92.8379</td> \n</tr>\n<tr>\n  <th>x3</th>    <td>-440.5288</td>  <td>6.8383</td>  <td>-64.4208</td> <td>0.0000</td> <td>-453.9320</td> <td>-427.1256</td>\n</tr>\n<tr>\n  <th>x4</th>    <td>496.4432</td>   <td>6.7301</td>   <td>73.7649</td> <td>0.0000</td> <td>483.2521</td>  <td>509.6342</td> \n</tr>\n<tr>\n  <th>x5</th>    <td>-190.1759</td>  <td>7.9864</td>  <td>-23.8124</td> <td>0.0000</td> <td>-205.8294</td> <td>-174.5223</td>\n</tr>\n<tr>\n  <th>x6</th>    <td>-209.3948</td>  <td>7.0214</td>  <td>-29.8225</td> <td>0.0000</td> <td>-223.1568</td> <td>-195.6328</td>\n</tr>\n<tr>\n  <th>x7</th>    <td>-766.0692</td>  <td>48.5550</td> <td>-15.7774</td> <td>0.0000</td> <td>-861.2378</td> <td>-670.9005</td>\n</tr>\n<tr>\n  <th>x8</th>     <td>20.1115</td>   <td>29.6365</td>  <td>0.6786</td>  <td>0.4974</td> <td>-37.9767</td>   <td>78.1996</td> \n</tr>\n<tr>\n  <th>x9</th>    <td>-26.8075</td>   <td>29.7693</td>  <td>-0.9005</td> <td>0.3679</td> <td>-85.1558</td>   <td>31.5408</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>12406.161</td>  <td>Durbin-Watson:</td>      <td>1.999</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>244618.648</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>0.894</td>       <td>Prob(JB):</td>        <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>    <td>14.526</td>    <td>Condition No.:</td>       <td>18</td>    \n</tr>\n</table>"
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled = sm.add_constant(X_train_scaled)\n",
    "X_test_scaled = sm.add_constant(X_test_scaled)\n",
    "model = sm.OLS(y_train, X_train_scaled)\n",
    "results = model.fit_regularized(L1_wt = 0.6,alpha=0.01)\n",
    "y_pred_elastic = results.predict(X_test_scaled)\n",
    "y_train_pred_elastic = results.predict(X_train_scaled)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_pred_elastic, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_pred_elastic, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_pred_elastic))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_pred_elastic))\n",
    "pinv_wexog, _ = pinv_extended(model.wexog)\n",
    "normalized_cov_params = np.dot(pinv_wexog, np.transpose(pinv_wexog))\n",
    "OLSResults(model, results.params, normalized_cov_params).summary2()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. [1 point] Explore the values of the parameters of the resulting models and compare the number of zero weights in them. Comment on the significance of the coefficients, overal model significance and other related factors from the results table"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1) В каждой обученной модели два незначимых коэффициента - x8 и x9.\n",
    "У данных коэффициентов p-value > 0.05, следовательно, гипотеза о незначимости\n",
    "данных коэффициентов не может быть отвергнута.\n",
    "2) Также в доверительных интервалах коэффициентов x8 и x9 во всех моделях содержится 0,\n",
    "что также говорит о незначимости данных коэффициентов.\n",
    "3) Также в каждой модели значение Prob(F-statistic) равно нулю. Это означает,\n",
    "что вероятность того, что все коэффициенты в моделях одновременно равны 0,\n",
    "равна 0, следовательно, все обученные модели являются значимыми.\n",
    "4) AIC - параметр, который показывает, насколько сильно переобучена модель.\n",
    "Чем меньше данный параметр, тем лучше модель. По такому критерию, среди обученных\n",
    "моделей лучшей является обычная линейная регрессия.\n",
    "5) По параметру BIC (то же самое, что AIC, только штрафует за ненужные параметры)\n",
    "также лучшей является линейная регрессия.\n",
    "6) В соответствии с параметром R-squared (чем он ближе к единице, тем точнее модель)\n",
    "наименее точной является модель Ridge."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. [1 point] Implement one of the elimination algorithms that were described in the Seminar_4 (Elimination by P-value, Forward elimination, Backward elimination), make conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significant features:  ['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x']\n"
     ]
    },
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary2.Summary'>\n\"\"\"\n                  Results: Ordinary least squares\n====================================================================\nModel:              OLS              Adj. R-squared:     0.885      \nDependent Variable: price            AIC:                744416.9518\nDate:               2022-10-16 20:26 BIC:                744486.3317\nNo. Observations:   43152            Log-Likelihood:     -3.7220e+05\nDf Model:           7                F-statistic:        4.758e+04  \nDf Residuals:       43144            Prob (F-statistic): 0.00       \nR-squared:          0.885            Scale:              1.8175e+06 \n---------------------------------------------------------------------\n         Coef.     Std.Err.     t      P>|t|     [0.025      0.975]  \n---------------------------------------------------------------------\nconst   3928.6813    6.4899  605.3528  0.0000   3915.9610   3941.4016\nx1      5259.0042   31.0446  169.4016  0.0000   5198.1562   5319.8522\nx2        76.7155    6.6560   11.5258  0.0000     63.6697     89.7613\nx3      -455.4584    6.8099  -66.8820  0.0000   -468.8059   -442.1109\nx4       491.4681    6.7020   73.3318  0.0000    478.3321    504.6041\nx5      -226.2530    7.1239  -31.7598  0.0000   -240.2159   -212.2900\nx6      -213.6785    6.9857  -30.5879  0.0000   -227.3706   -199.9864\nx7     -1340.6484   30.9523  -43.3134  0.0000  -1401.3155  -1279.9813\n--------------------------------------------------------------------\nOmnibus:            11265.398      Durbin-Watson:         1.999     \nProb(Omnibus):      0.000          Jarque-Bera (JB):      360683.897\nSkew:               0.610          Prob(JB):              0.000     \nKurtosis:           17.111         Condition No.:         10        \n====================================================================\n\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<tr>\n        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>      <td>0.885</td>   \n</tr>\n<tr>\n  <td>Dependent Variable:</td>       <td>price</td>             <td>AIC:</td>         <td>744416.9518</td>\n</tr>\n<tr>\n         <td>Date:</td>        <td>2022-10-16 20:26</td>        <td>BIC:</td>         <td>744486.3317</td>\n</tr>\n<tr>\n   <td>No. Observations:</td>        <td>43152</td>        <td>Log-Likelihood:</td>   <td>-3.7220e+05</td>\n</tr>\n<tr>\n       <td>Df Model:</td>              <td>7</td>           <td>F-statistic:</td>      <td>4.758e+04</td> \n</tr>\n<tr>\n     <td>Df Residuals:</td>          <td>43144</td>      <td>Prob (F-statistic):</td>    <td>0.00</td>    \n</tr>\n<tr>\n      <td>R-squared:</td>            <td>0.885</td>            <td>Scale:</td>        <td>1.8175e+06</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>Coef.</th>   <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th>   <th>[0.025</th>     <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>  <td>3928.6813</td>  <td>6.4899</td>  <td>605.3528</td> <td>0.0000</td>  <td>3915.9610</td>  <td>3941.4016</td>\n</tr>\n<tr>\n  <th>x1</th>     <td>5259.0042</td>  <td>31.0446</td> <td>169.4016</td> <td>0.0000</td>  <td>5198.1562</td>  <td>5319.8522</td>\n</tr>\n<tr>\n  <th>x2</th>      <td>76.7155</td>   <td>6.6560</td>   <td>11.5258</td> <td>0.0000</td>   <td>63.6697</td>    <td>89.7613</td> \n</tr>\n<tr>\n  <th>x3</th>     <td>-455.4584</td>  <td>6.8099</td>  <td>-66.8820</td> <td>0.0000</td>  <td>-468.8059</td>  <td>-442.1109</td>\n</tr>\n<tr>\n  <th>x4</th>     <td>491.4681</td>   <td>6.7020</td>   <td>73.3318</td> <td>0.0000</td>  <td>478.3321</td>   <td>504.6041</td> \n</tr>\n<tr>\n  <th>x5</th>     <td>-226.2530</td>  <td>7.1239</td>  <td>-31.7598</td> <td>0.0000</td>  <td>-240.2159</td>  <td>-212.2900</td>\n</tr>\n<tr>\n  <th>x6</th>     <td>-213.6785</td>  <td>6.9857</td>  <td>-30.5879</td> <td>0.0000</td>  <td>-227.3706</td>  <td>-199.9864</td>\n</tr>\n<tr>\n  <th>x7</th>    <td>-1340.6484</td>  <td>30.9523</td> <td>-43.3134</td> <td>0.0000</td> <td>-1401.3155</td> <td>-1279.9813</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td>Omnibus:</td>    <td>11265.398</td>  <td>Durbin-Watson:</td>      <td>1.999</td>  \n</tr>\n<tr>\n  <td>Prob(Omnibus):</td>   <td>0.000</td>   <td>Jarque-Bera (JB):</td> <td>360683.897</td>\n</tr>\n<tr>\n       <td>Skew:</td>       <td>0.610</td>       <td>Prob(JB):</td>        <td>0.000</td>  \n</tr>\n<tr>\n     <td>Kurtosis:</td>    <td>17.111</td>    <td>Condition No.:</td>       <td>10</td>    \n</tr>\n</table>"
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = list(X_train.columns)\n",
    "while len(features) > 0:\n",
    "    X_train_model = X_train[features]\n",
    "    X_train_model = scaler.fit_transform(X_train_model)\n",
    "    X_train_model = sm.add_constant(X_train_model)\n",
    "    model = sm.OLS(y_train, X_train_model)\n",
    "    results = model.fit()\n",
    "    p_values = results.pvalues\n",
    "    max_p_value = np.max(p_values)\n",
    "    max_index = np.argmax(p_values)\n",
    "    if max_p_value > 0.05:\n",
    "        features.remove(X_train.columns[max_index - 1])\n",
    "    else:\n",
    "        break\n",
    "print('Significant features: ', features)\n",
    "results.summary2()\n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Выводы:\n",
    "1) Незначимыми являются параметры y и z.\n",
    "2) Значение F-статистики после elimination стало больше, следовательно,\n",
    "модель линейной регрессии после elimination стала значимее."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. [1 point] Find the best (in terms of RMSE) $\\alpha$ for Lasso regression using cross-validation with 4 folds. You must select values from range $[10^{-4}, 10^{3}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha -  2.682695795279722\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(alpha=0.01)\n",
    "alphas = np.logspace(-4, 3)\n",
    "model = GridSearchCV(lasso, {'alpha':alphas}, scoring='neg_mean_squared_error', cv=4)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print('Best alpha - ', model.best_params_['alpha'])\n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "#### 6. [3.5 points] Implement a Ridge regression model for the MSE loss function, trained by gradient descent.\n",
    "\n",
    "All calculations must be vectorized, and python loops can only be used for gradient descent iterations. As a stop criterion, you must use (simultaneously):\n",
    "\n",
    "* checking for the Absolute-value norm of the weight difference on two adjacent iterations (for example, less than some small number of the order of $10^{-6}$, set by the `tolerance` parameter);\n",
    "* reaching the maximum number of iterations (for example, 10000, set by the `max_iter` parameter).\n",
    "\n",
    "You need to implement:\n",
    "\n",
    "* Full gradient descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "* Stochastic Gradient Descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$\n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\, $ is the estimate of the gradient over the batch of objects selected randomly.\n",
    "\n",
    "* Momentum method:\n",
    "\n",
    "$$\n",
    "h_0 = 0, \\\\\n",
    "h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
    "w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "* Adagrad method:\n",
    "\n",
    "$$\n",
    "G_0 = 0, \\\\\n",
    "G_{k + 1} = G_{k} + (\\nabla_{w} Q(w_{k+1}))^2, \\\\\n",
    "w_{k + 1} = w_{k} - \\eta * \\frac{\\nabla_{w} Q(w_{k+1})}{\\sqrt{G_{k+1} + \\epsilon}}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "To make sure that the optimization process really converges, we will use the `loss_history` class attribute. After calling the `fit` method, it should contain the values of the loss function for all iterations, starting from the first one (before the first step on the anti-gradient).\n",
    "\n",
    "You need to initialize the weights with a random vector from normal distribution. The following is a template class that needs to contain the code implementing all variations of the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinReg(BaseEstimator):\n",
    "    def __init__(self, delta=0.4, gd_type='Momentum',\n",
    "                 tolerance=1e-4, max_iter=1000, w0=None, eta=1e-2, alpha=1e-3, epsilon = 1e-8, reg_cf = 1e-2 ):\n",
    "        \"\"\"\n",
    "        gd_type: str\n",
    "            'GradientDescent', 'StochasticDescent', 'Momentum', 'Adagrad'\n",
    "        delta: float\n",
    "            proportion of object in a batch (for stochastic GD)\n",
    "        tolerance: float\n",
    "            for stopping gradient descent\n",
    "        max_iter: int\n",
    "            maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d)\n",
    "            init weights\n",
    "        eta: float\n",
    "            learning rate\n",
    "        alpha: float\n",
    "            momentum coefficient\n",
    "        reg_cf: float\n",
    "            regularization coefficient\n",
    "        epsilon: float\n",
    "            numerical stability\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.delta = delta\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.eta = eta\n",
    "        self.reg_cf = reg_cf\n",
    "        self.loss_history = None # list of loss function values at each training iteration\n",
    "\n",
    "    def full_gradient_descent(self, X, y):\n",
    "        iterations = 0\n",
    "        self.w = self.w0.copy()\n",
    "        w_prev = self.w0.copy()\n",
    "        while iterations < self.max_iter and (np.linalg.norm(self.w - w_prev) - self.tolerance >= 0 or iterations == 0):\n",
    "            iterations += 1\n",
    "            w_prev = self.w.copy()\n",
    "            gradient = self.calc_gradient(X, y)\n",
    "            self.w -= self.eta * gradient\n",
    "            self.loss_history.append(self.calc_loss(X, y))\n",
    "\n",
    "    def stochastic_gradient_descent(self, X, y):\n",
    "        iterations = 0\n",
    "        self.w = self.w0.copy()\n",
    "        w_prev = self.w0.copy()\n",
    "        batch_size = int(self.delta * X.shape[0])\n",
    "        while iterations < self.max_iter and (np.linalg.norm(self.w - w_prev) - self.tolerance >= 0 or iterations == 0):\n",
    "            sample = np.random.randint(y.shape[0], size=batch_size)\n",
    "            iterations += 1\n",
    "            w_prev = self.w\n",
    "            f = X[sample, :].dot(self.w)\n",
    "            error = f - y.iloc[sample]\n",
    "            gradient = (2 * X[sample, :].T.dot(error) + 2 * self.reg_cf * self.w) / batch_size\n",
    "            self.w -= self.eta * gradient\n",
    "            self.loss_history.append(self.calc_loss(X, y))\n",
    "\n",
    "    def momentum(self, X, y):\n",
    "        iterations = 0\n",
    "        self.w = self.w0.copy()\n",
    "        w_prev = self.w0.copy()\n",
    "        h = np.zeros(self.w.shape[0])\n",
    "        while iterations < self.max_iter and (np.linalg.norm(self.w - w_prev) - self.tolerance >= 0 or iterations == 0):\n",
    "            iterations += 1\n",
    "            w_prev = self.w\n",
    "            gradient = self.calc_gradient(X, y)\n",
    "            h = self.alpha * h + self.eta * gradient\n",
    "            self.w = self.w - h\n",
    "            self.loss_history.append(self.calc_loss(X, y))\n",
    "\n",
    "\n",
    "    def adagrad(self, X, y):\n",
    "        iterations = 0\n",
    "        self.w = self.w0.copy()\n",
    "        w_prev = self.w0.copy()\n",
    "        G = np.zeros(self.w.shape[0])\n",
    "        while iterations < self.max_iter and (np.linalg.norm(self.w - w_prev) - self.tolerance >= 0 or iterations == 0):\n",
    "            iterations += 1\n",
    "            w_prev = self.w\n",
    "            gradient = self.calc_gradient(X, y)\n",
    "            G += np.square(gradient)\n",
    "            self.w = self.w - (self.eta / np.sqrt(G + self.epsilon) * gradient)\n",
    "            self.loss_history.append(self.calc_loss(X, y))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        self.loss_history = []\n",
    "        self.w0 = np.random.normal(0, 1, X.shape[1])\n",
    "        if self.gd_type == 'GradientDescent':\n",
    "            self.full_gradient_descent(X, y)\n",
    "        if self.gd_type == 'StochasticDescent':\n",
    "            self.stochastic_gradient_descent(X, y)\n",
    "        if self.gd_type == 'Momentum':\n",
    "            self.momentum(X, y)\n",
    "        if self.gd_type == 'Adagrad':\n",
    "            self.adagrad(X, y)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        return X.dot(self.w)\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d) (l can be equal to 1 if stochastic)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "        f = X.dot(self.w)\n",
    "        error = f - y\n",
    "        gradient = (2 * X.T.dot(error) + 2 * self.reg_cf * self.w) / y.shape[0]\n",
    "        return gradient\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\"\n",
    "        return ((X.dot(self.w) - y).T.dot(X.dot(self.w) - y) +\n",
    "                self.reg_cf * np.sum(pow(self.w, 2)))/X.shape[0]\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. [1 points] Train and validate \"hand-written\" models on the same data, and compare the quality with the Sklearn or StatsModels methods. Investigate the effect of the `max_iter` and `alpha` parameters on the optimization process. Is it consistent with your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momentum\n",
      "Train RMSE = 1418.1162\n",
      "Test RMSE = 1433.9168\n",
      "Train R2 = 0.8731\n",
      "Test R2 = 0.8730\n",
      "Predicted coefficients\n",
      "[3928.68037249 3152.61212376   87.40254009 -405.72489451  516.52011589\n",
      " -116.43465276 -198.72674328  517.30420985  148.40041029   69.40691828]\n"
     ]
    }
   ],
   "source": [
    "linreg = LinReg()\n",
    "linreg.fit(X_train_scaled, y_train)\n",
    "momentum_loss_history = linreg.loss_history\n",
    "y_pred = linreg.predict(X_train_scaled)\n",
    "y_test_pred = linreg.predict(X_test_scaled)\n",
    "print('Momentum')\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_pred, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_pred, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_pred))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_pred))\n",
    "print('Predicted coefficients')\n",
    "print(linreg.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent\n",
      "Train RMSE = 1418.1781\n",
      "Test RMSE = 1433.9839\n",
      "Train R2 = 0.8731\n",
      "Test R2 = 0.8730\n",
      "Predicted coefficients\n",
      "[3928.68037235 3151.68454825   87.39894107 -405.69622458  516.52406867\n",
      " -116.41033836 -198.70210263  516.6502787   149.66320111   69.72192029]\n"
     ]
    }
   ],
   "source": [
    "linreg = LinReg(gd_type='GradientDescent')\n",
    "linreg.fit(X_train_scaled, y_train)\n",
    "gradient_descent_loss_history = linreg.loss_history\n",
    "y_pred = linreg.predict(X_train_scaled)\n",
    "y_test_pred = linreg.predict(X_test_scaled)\n",
    "print('Gradient descent')\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_pred, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_pred, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_pred))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_pred))\n",
    "print('Predicted coefficients')\n",
    "print(linreg.w)\n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic descent\n",
      "Train RMSE = 1418.3055\n",
      "Test RMSE = 1434.1346\n",
      "Train R2 = 0.8731\n",
      "Test R2 = 0.8730\n",
      "Predicted coefficients\n",
      "[3929.14168009 3150.87678481   86.80801541 -405.95012177  516.10419625\n",
      " -116.09678976 -198.74264534  516.68709841  156.13231985   68.21774649]\n"
     ]
    }
   ],
   "source": [
    "linreg = LinReg(gd_type='StochasticDescent', tolerance=1e-100000)\n",
    "linreg.fit(X_train_scaled, y_train)\n",
    "stochastic_descent_loss_history = linreg.loss_history\n",
    "y_pred = linreg.predict(X_train_scaled)\n",
    "y_test_pred = linreg.predict(X_test_scaled)\n",
    "print('Stochastic descent')\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_pred, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_pred, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_pred))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_pred))\n",
    "print('Predicted coefficients')\n",
    "print(linreg.w)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best eta for adagrad -  {'eta': 1526.4179671752333}\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(LinReg(gd_type='Adagrad'), {'eta' : np.logspace(2, 4)},\n",
    "                           cv = 4, scoring='neg_root_mean_squared_error')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_eta = grid_search.best_params_\n",
    "print('Best eta for adagrad - ', best_eta)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adagrad\n",
      "Train RMSE = 1348.0143\n",
      "Test RMSE = 1370.3796\n",
      "Train R2 = 0.8853\n",
      "Test R2 = 0.8840\n",
      "Predicted coefficients\n",
      "[ 3928.68037897  5234.21544414    76.76673304  -455.07401451\n",
      "   491.87031492  -223.04800188  -213.47208723 -1327.31764988\n",
      "    25.82704136   -13.98538358]\n"
     ]
    }
   ],
   "source": [
    "linreg = LinReg(gd_type='Adagrad', eta=1526.4179671752333)\n",
    "linreg.fit(X_train_scaled, y_train)\n",
    "adagrad_loss_history = linreg.loss_history\n",
    "y_pred = linreg.predict(X_train_scaled)\n",
    "y_test_pred = linreg.predict(X_test_scaled)\n",
    "print('Adagrad')\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_pred, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_test_pred, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_pred))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_test_pred))\n",
    "print('Predicted coefficients')\n",
    "print(linreg.w)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 1347.9933\n",
      "Test RMSE = 1370.5905\n",
      "Train R2 = 0.8853\n",
      "Test R2 = 0.8840\n"
     ]
    }
   ],
   "source": [
    "model = Ridge(alpha=0.01)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred_ridge = model.predict(X_test_scaled)\n",
    "y_train_pred_ridge = model.predict(X_train_scaled)\n",
    "\n",
    "print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_train_pred_ridge, squared=False))\n",
    "print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_pred_ridge, squared=False))\n",
    "print(\"Train R2 = %.4f\" % r2_score(y_train, y_train_pred_ridge))\n",
    "print(\"Test R2 = %.4f\" % r2_score(y_test, y_pred_ridge))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Выводы:\n",
    "1) При tolerance, равном 1e-4, алгоритм стохастического градиентного спуска очень\n",
    "быстро заканчивается, так как выход из цикла происходит после одной итерации. Качество\n",
    "обучения при этом низкое, поэтому можно уменьшить параметр tolerance и сделать его,\n",
    "например, 1e-10. В этом случае качество модели становится лучше.\n",
    "2) При eta = 0.01 и обучении алгоритмом adagrad функция ошибки убывает очень медленно\n",
    "в силу деления eta на каждой итерации. Эту проблему удалось решить, увеличив learning\n",
    "rate. Как и ожидалось, качество модели стало намного лучше.\n",
    "3) Также при увеличении max_iter качество модели улучшается, однако алгоритм дольше работает.\n",
    "4) Наиболее точной моделью, судя по параметру R^2, является модель Ridge из sklearn,\n",
    "ее R^2 ближе всех к единице. (При дефолтных параметрах)\n",
    "5) По параметрам RMSE также лучшей является модель Ridge из sklearn, у нее наименьшее RMSE.\n",
    "6) При дефолтных параметрах R^2 для модели, обученной алгоритмами adagrad и stochastic\n",
    "gradient descent, отрицательный, именно поэтому необходимы изменения параметров, описанные\n",
    "в пунктах 1 и 2."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. [1 points] Plot graphs (on the same picture) of the dependence of the loss function value on the iteration number for Full GD, SGD, Momentum and Adagrad. Draw conclusions about the rate of convergence of various modifications of gradient descent.\n",
    "\n",
    "Don't forget about what *beautiful* graphics should look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0, 0.5, 'Loss function value')"
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = len(momentum_loss_history)\n",
    "list_of_indexes = np.arange(count)\n",
    "plt.plot(list_of_indexes, momentum_loss_history)\n",
    "count = len(gradient_descent_loss_history)\n",
    "list_of_indexes = np.arange(count)\n",
    "plt.plot(list_of_indexes, gradient_descent_loss_history)\n",
    "count = len(stochastic_descent_loss_history)\n",
    "list_of_indexes = np.arange(count)\n",
    "plt.plot(list_of_indexes, stochastic_descent_loss_history)\n",
    "count = len(adagrad_loss_history)\n",
    "list_of_indexes = np.arange(count)\n",
    "plt.plot(list_of_indexes, adagrad_loss_history)\n",
    "plt.title('Dependence of loss function and iteration number')\n",
    "plt.legend(['Momentum', 'Full gradient descent', 'Stochastic descent', 'Adagrad'])\n",
    "plt.xlabel('Number of iteration')\n",
    "plt.ylabel('Loss function value')\n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1) С каждой новой итерацией значение функции потерь для моделей, обученных\n",
    "методами полного, стохастического градиентного спуска и методом momentum, при\n",
    "learning rate = 0.01 становится меньше. График зависимости довольно быстро убывает.\n",
    "Менее чем за 100 итераций значение функции потерь сильно падает. По графику явно заметно,\n",
    "что значения функции сходятся к какому-то минимуму.\n",
    "2) В отличие от других графиков, график зависимости значения функции потерь от номера\n",
    "итерации для модели, обученной методом adagrad при заданном learning rate = 0.01 убывает очень медленно,\n",
    "и за большое количество итераций не сходится к конкретному значению минимума. Поэтому было принято решение\n",
    "увеличить learning rate, а для поиска наиболее оптимального learning rate была использована\n",
    "кросс-валидация. Благодаря этому, график зависимости значения функции ошибки от номера итерации убывает\n",
    "быстрее всех. Значения функции при этом сходятся примерно к тому же минимуму, что и значения функции при\n",
    "других методах обучения.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}